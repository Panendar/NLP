{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a253e3",
   "metadata": {},
   "source": [
    "### NOT MUCH USEFUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8445e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding in NLP is a method to represent words as binary vectors, where each vector has a length equal to the size of the vocabulary, \n",
    "# and only one element is '1' (indicating the presence of the word) while all other elements are '0'. \n",
    "# Ex: happy, sad, angry represented as [1,0,0], [0,1,0], [0,0,1] respectively.\n",
    "\n",
    "import numpy as np\n",
    "corpus = \"I love programming. Programming is fun. I love fun.\"\n",
    "vocab = set()\n",
    "for sentence in corpus.split('.'):\n",
    "    words = sentence.lower().split()\n",
    "    for word in words:\n",
    "        vocab.add(word)\n",
    "\n",
    "# print(\"Vocabulary: \", set(vocab))\n",
    "# word_to_int = {word: i for i, word in enumerate(set(vocab))}\n",
    "# print(\"Word to Integer Mapping: \", word_to_int)\n",
    "\n",
    "vectors = []\n",
    "for sentences in corpus.split('.'):\n",
    "    words = sentences.lower().split()\n",
    "    sentence_vector = np.zeros(len(vocab))\n",
    "    for word in words:\n",
    "        index = list(vocab).index(word)\n",
    "        sentence_vector[index] = 1\n",
    "    vectors.append(sentence_vector)\n",
    "print(\"One Hot Encoded Vectors: \\n\", np.array(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f850e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words (BoW) is a text representation technique that converts text into fixed-length vectors based on word frequency,\n",
    "import nltk\n",
    "import re\n",
    "text = \"\"\"Beans. I wa strying to explain to somebody as we were flying in, that's corn.  That's beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction.\"\"\"\n",
    "\n",
    "sent = nltk.sent_tokenize(text)\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "for i in range(len(sent)):\n",
    "    sent[i] = sent[i].lower()\n",
    "    sent[i] = re.sub(r'\\W', ' ', sent[i])\n",
    "    sent[i] = re.sub(r'\\s+', ' ', sent[i])\n",
    "    print(f\"Sentence {i+1}: {sent[i]}\")\n",
    "\n",
    "# counting word frequency\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sent)\n",
    "print(\"Bag of Words Representation:\\n\", X.toarray())\n",
    "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4195ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF is a statistical measure used to evaluate the importance of a word in a word document relative to a collection of documents (corpus).\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "d0 = 'Geeks for geeks'\n",
    "d1 = 'Geeks'\n",
    "d2 = 'r2j'\n",
    "string = [d0, d1, d2]\n",
    "tfidf = TfidfVectorizer()\n",
    "result = tfidf.fit_transform(string)\n",
    "print(\"TF-IDF Representation:\\n\", result.toarray())\n",
    "print(\"Feature Names:\\n\", tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97194d9",
   "metadata": {},
   "source": [
    "### NGRAM MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b220b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Gram Language Model: predict the probability of a word given the previous nâˆ’1 words. For example, a trigram model uses the preceding two words to predict the next word:\n",
    "#Goal: estimate conditional probabilities from counts in a corpus, then use them for scoring, perplexity, or generation.\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "text = \"I love programming. Programming is fun. I love fun.\"\n",
    "tokens = nltk.word_tokenize(text.lower())\n",
    "tokenized = [word for word in tokens if word.isalnum()] # isalnum checks it has only letters and numbers\n",
    "# print(\"Tokens: \", tokenized)\n",
    "\n",
    "# unigrams\n",
    "# unigrams = list(ngrams(tokenized,1))\n",
    "# # print(\"Unigrams: \", unigrams)\n",
    "# count_unigrams = Counter(unigrams)\n",
    "# total_unigrams = sum(count_unigrams.values()) # or len(unigrams)\n",
    "# vocab = set(u[0] for u in unigrams)\n",
    "# v = len(vocab)\n",
    "# def unigram_prob(word):\n",
    "#     # adding +1 and V for (Laplace) smoothing\n",
    "#     return (count_unigrams.get((word,),0) + 1) / (total_unigrams + v) # i can use count_unigrams(word) = count(word) in unigrams too\n",
    "# print(\"Unigram Probability of 'love': \", unigram_prob('love'))\n",
    "\n",
    "\n",
    "\n",
    "# Bigrams\n",
    "# bigrams = list(ngrams(tokenized,2))\n",
    "# # print(\"Bigrams: \", bigrams)\n",
    "# count_bigrams = Counter(bigrams)\n",
    "# bigram_context_count = Counter(ngrams(tokenized,1))\n",
    "# total_bigrams = sum(count_bigrams.values()) # or len(bigrams)\n",
    "# vocab = set([word for bigram in bigrams for word in bigrams])\n",
    "# v = len(vocab)\n",
    "# def bigram_prob(word1, word2):\n",
    "#     # P(word | prev_word) = count(prev_word, word) / count(prev_word)\n",
    "#     num = count_bigrams.get((word1, word2),0)               # if want smoothing/ laplace (count(prev_word, word) + 1) / (count(prev_word) + V)\n",
    "#     denom = bigram_context_count.get((word1,),0)\n",
    "#     return num / denom if denom > 0 else 0\n",
    "# print(\"Bigram Probability of 'programming' given 'love': \", bigram_prob('love', 'programming'))\n",
    "\n",
    "\n",
    "# Trigrams\n",
    "trigrams = list(ngrams(tokenized,3))\n",
    "# print(\"Trigrams: \", trigrams)\n",
    "count_trigrams = Counter(trigrams)\n",
    "trigram_context_count = Counter(ngrams(tokenized,2))\n",
    "total_trigrams = sum(count_trigrams.values()) # or len(trigrams)\n",
    "vocab = set([word for trigram in trigrams for word in trigrams])\n",
    "v = len(vocab)\n",
    "def trigram_prob(word1, word2, word3):\n",
    "    # P(word3 | word1, word2) = count(word1, word2, word3) / count(word1, word2)\n",
    "    num = count_trigrams.get((word1,word2,word3), 0)\n",
    "    denom = trigram_context_count.get((word1,word2),0)\n",
    "    return num / denom if denom > 0 else 0\n",
    "print(\"Trigram Probability of 'fun' given 'I love': \", trigram_prob('i', 'love', 'fun'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a63856",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a91a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this we have to train the model, there are mainly 2 types of Word2Vec models:\n",
    "# 1.Continuous Bag of Words (CBOW) 2. Skip-Gram\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "corpus = [\n",
    "    \"king queen royal family\",\n",
    "    \"man woman child home\",\n",
    "    \"apple mango banana fruit\",\n",
    "    \"dog cat animal pet\",\n",
    "    \"car bus train vehicle\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [simple_preprocess(sentence) for sentence in corpus]\n",
    "# print(tokenized_corpus)\n",
    "\n",
    "model_cbow = Word2Vec(sentences=tokenized_corpus, vector_size =150, window =5, min_count=1, sg =0) # sg=0 for CBOW\n",
    "model_skipgram = Word2Vec(sentences=tokenized_corpus, vector_size =150, window =5, min_count=1, sg =1) # sg=1 for Skip-Gram\n",
    "\n",
    "vec = model_skipgram.wv[\"king\"]\n",
    "# print(vec)\n",
    "model_skipgram.wv.most_similar(\"king\")\n",
    "model_skipgram.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c451ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('glove-wiki-gigaword-100')  # 100-dimensional GloVe vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d69de3",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7efb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q glove-python-binary\n",
    "\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "corpus = Corpus()\n",
    "sentences = [\n",
    "    \"king queen royal family\",\n",
    "    \"man woman child home\",\n",
    "    \"apple mango banana fruit\",\n",
    "    \"dog cat animal pet\",\n",
    "    \"car bus train vehicle\"\n",
    "]\n",
    "\n",
    "sentences = [s.split() for s in sentences]\n",
    "corpus.fit(sentences, window=5)\n",
    "\n",
    "# (optional) train a Glove model to obtain vectors\n",
    "glove = Glove(no_components=50, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=False)\n",
    "glove.add_dictionary(corpus.dictionary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
