# üìö NLP Breakdown - Learning Natural Language Processing

Welcome to my **Natural Language Processing (NLP) Learning Journey**! This project is designed to understand and compare different NLP techniques and libraries through hands-on experimentation.

## üéØ Learning Objectives

This project helps me understand:

- **Text Preprocessing** techniques
- **Tokenization** methods (sentence and word level)
- **Stop word removal** for text cleaning
- **Stemming vs Lemmatization** differences
- **Part-of-Speech (POS) tagging**
- **SpaCy vs NLTK** library comparisons

## üõ†Ô∏è Technologies Used

### Libraries

- **SpaCy** - Advanced NLP library with pre-trained models
- **NLTK** - Natural Language Toolkit for text processing
- **Python 3.13+** - Programming language

### Models & Data

- **en_core_web_sm** - SpaCy's English language model
- **NLTK Corpora** - stopwords, wordnet, punkt tokenizers

## üìã Prerequisites

Make sure you have the following installed:

```bash
# Install Python packages
pip install spacy nltk

# Download SpaCy English model
python -m spacy download en_core_web_sm

# Download NLTK data
python -c "import nltk; nltk.download('punkt'); nltk.download('punkt_tab'); nltk.download('stopwords'); nltk.download('wordnet')"
```

## üöÄ Usage

Run the script and enter your text for analysis:

```bash
python nlp_breakdown.py
```

### Example Input

```
The children were running quickly through the beautiful gardens. They stopped to admire the colorful flowers blooming magnificently.
```

## üìä What the Script Does

### 1. **SpaCy Analysis**

- Performs **lemmatization** with grammatical context
- Provides **Part-of-Speech tagging**
- Shows word ‚Üí lemma ‚Üí POS format

### 2. **NLTK Text Processing**

- **Sentence Tokenization**: Splits text into sentences
- **Word Tokenization**: Breaks sentences into individual words
- **Stop Word Removal**: Filters out common words (the, and, is, etc.)
- **Unique Word Analysis**: Counts distinct meaningful words
- **Shows Removed Words**: Displays filtered-out terms

### 3. **Stemming vs Lemmatization Comparison**

- **Stemming**: Aggressive suffix removal (may create non-words)
- **Lemmatization**: Dictionary-based word reduction (produces real words)

## üîç Key Learning Differences

| Aspect       | Stemming              | Lemmatization     |
| ------------ | --------------------- | ----------------- |
| **Speed**    | ‚ö° Fast               | üêå Slower         |
| **Accuracy** | üìâ Lower              | üìà Higher         |
| **Output**   | May produce non-words | Always real words |
| **Context**  | Rule-based            | Grammar-aware     |
| **Use Case** | Search engines        | Text analysis     |

### Example Comparisons

- `running` ‚Üí Stem: `run` vs Lemma: `running`
- `children` ‚Üí Stem: `children` vs Lemma: `child`
- `quickly` ‚Üí Stem: `quickli` vs Lemma: `quickly`
- `beautiful` ‚Üí Stem: `beauti` vs Lemma: `beautiful`

## üìù Sample Output Structure

```
Using Spacy...
word -> lemma -> POS_TAG

Using NLTK...
Original Sentence: [input text]
Sentence Tokenization: [list of sentences]
Word Tokenization: [list of words]
Filtered Sentence: [words after stop word removal]
Analysis (unique words): [unique content words] -> Count: X
Removed Words: [stop words and punctuation]

===============Stemming==============
word -> stemmed_form

===============Lemmatization==============
word -> lemmatized_form
```

## üìö Learning Notes

### When to Use What?

- **Stemming**: Information retrieval, search engines, keyword matching
- **Lemmatization**: Sentiment analysis, text classification, language understanding
- **SpaCy**: Complex NLP tasks requiring accuracy and linguistic features
- **NLTK**: Educational purposes, prototyping, basic text processing

### Key Insights

1. **Lemmatization is generally better** for most NLP applications
2. **SpaCy provides richer linguistic information** than basic NLTK processing
3. **Stop word removal is crucial** for focusing on meaningful content
4. **Tokenization is the foundation** of most NLP tasks

## üéì Learning Progress

- [x] Basic text tokenization
- [x] Stop word filtering
- [x] Stemming implementation
- [x] Lemmatization comparison
- [x] SpaCy integration
- [x] POS tagging understanding
- [ ] Named Entity Recognition (NER) - _Next goal_
- [ ] Text classification - _Future learning_
- [ ] Sentiment analysis - _Planned_

## ü§ù Educational Purpose

This project is part of my journey to understand:

- **Text preprocessing pipelines**
- **NLP library ecosystems**
- **Preprocessing impact on downstream tasks**
- **Performance vs accuracy trade-offs**

## üìû Notes for Future Development

### Potential Enhancements

- Add **Named Entity Recognition**
- Include **dependency parsing**
- Implement **text similarity metrics**
- Add **n-gram analysis**
- Create **word frequency distributions**
- Include **sentiment analysis**

### Learning Resources Referenced

- SpaCy Documentation
- NLTK Book
- NLP with Python tutorials
- Various NLP research papers

---

**Happy Learning!** üöÄüìñ

_This project demonstrates fundamental NLP concepts and serves as a foundation for more advanced natural language processing techniques._
